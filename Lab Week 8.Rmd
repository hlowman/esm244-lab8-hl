---
title: "Lab Week 8"
author: "Heili Lowman"
date: "3/8/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load packages.

```{r packages}

library(tidyverse)
library(sf)
library(tmap)
library(leaflet)
library(spatstat)
library(maptools)

```

# Column graph of Texas oil spills.

```{r data}

spills <- read_csv("oil_spills.csv") #Load data.

df <- spills %>% 
  filter(`Accident State`=="TX" & `Accident Year` < 2017) %>%  # Because the name has a space in it it must be bounded by the little backwards apostrophes. This chooses only Texan data before 2017.
  group_by(`Accident Year`) %>% 
  summarise(Loss = sum(`Net Loss (Barrels)`))

colnames(df) <- c("Year", "Loss") # Rename columns.  

ggplot(df, aes(x = Year, y = Loss)) +
  geom_col() # Creates a column graph.

```

# Leaflet plot of spill locations in TX in 2016.

```{r leaflet}

df_loc <- spills %>% 
  filter(`Accident State` == "TX" & `Accident Year` == 2016) %>% 
  select(Latitude, Longitude, `Net Loss (Barrels)`) # Create a new dataframe filtering 2016 Texas data and keep only lat, long, and loss columns.

colnames(df_loc) <- c("latitude", "longitude", "net_loss") # Changes the column names.

oil_sf <- st_as_sf(df_loc, coords = c("longitude", "latitude"), crs = 4326) # Convert data frame information into shape data. Remember to always list longitude first, otherwise your map will appear flipped by 90 degrees.

leaflet(oil_sf) %>% 
  addTiles() %>% #Adding face map.
  addMarkers() #Net Loss is the only variable, so you don't have to specify.
  

```

# Tmap plot with the Texas state shapefile and oil spills as points.

```{r tmap}

states <- st_read(dsn = ".", layer = "states" ) # The period refers to items in your working directory that contain the name "states" and sets all those files to something called "states".

tex_border <- states %>% # From the states shapefile in our folder.
  filter(STATE_NAME == "Texas") %>%  # Filter out only the state of Texas.
  st_transform(4326) # Coordinate reference system.

plot(tex_border)

tm_shape(tex_border) + # Create a map using tmap.
  tm_polygons() + # Plot the TX border as a polygon.
  tm_shape(oil_sf) + # Add in oil spill points from above.
  tm_dots(size = 0.3) # Edits size of points.

```

# Convert the data to spatial points patterns (combination of the point data and the bounding window).

```{r point_pattern}

spill_spatial <- as(oil_sf, "Spatial") # Converting from simple features back to a data frame.
spill_ppp <- as(spill_spatial, "ppp") # Convert the new data frame to a point pattern.

tx_sp <- as(tex_border, "Spatial") # Convert texas border data back into a data frame.
tx_owin <-as(tx_sp, "owin") # Sets the TX boundary as the window.

all_ppp <- ppp(spill_ppp$x, spill_ppp$y, window = tx_owin) # Omits points that don't align between the points pattern and the window we define.

```

# A density plot:

```{r density}

plot(density(all_ppp, sigma = 0.4)) # Careful, when you change sigma, the output changes drastically.

```

# CSR Tests for point patterns:

## Quadrat test for spatial evenness

```{r quadrat}

# Are oil spills evenly distributed throughout the state?

oil_qt <- quadrat.test(all_ppp, nx = 5, ny = 5) # nx is 5 horizontal regions, ny is 5 vertical regions.
oil_qt
# X2 = 589.36, df = 17, p-value < 2.2e-16
# Reject the null hypothesis, Retain the alternative hypothesis that the data is NOT CSR, so it is NOT spatially evenly distributed.

plot(all_ppp) # Plots all points as created in previous code chunk.
plot(oil_qt, add = TRUE, cex = 0.4) # Adds in quadrats and counts.

# Looking at the square full quadrat in the middle: 14.5 = the number of points there should be if the data was truly CSR (notice it's the same to the other complete quadrat to it's right, wherease the quadrat to the left is missing a small chunk so the predicted number of events is 14 instead, if the data was truly evenly distributed). the other full number is the actual number of events (4) and the remaining number at the bottom is the standard deviation between the actual and expected (if CSR) number of events.

```

## G function for Nearest Neighbor Analysis
Review all the code below here. This was late in the class and my brain was mushy.

```{r G_fun}

r <- seq(0,1, by = 0.01) # Creates a sequence.

oil_gfun <- envelope(all_ppp, fun = Gest, r = r, nsim = 100) # Runs a hundred simulations.

ggplot(oil_gfun, aes(x = r, y = obs)) +
  geom_line(color = "black") +
  geom_line(aes(x = r, y = theo), color = "red") +
  ggtitle("G Function Graph")

# This shows us that our observed data has a higher proportion of point pairs with nearest neighbors at shorter distances compared to CSR (evenly distributed) data.

```

## Nearest neighbor using the L-function (Ripley's K, standardized)

```{r L_fun}

r2 <- seq(0,3, by = 0.5)

oil_lfun <- envelope(all_ppp, fun = Lest, r = r2, nsim = 20, global = TRUE) # This is looking at every single point that has an event and makes increasing bubbles around it until it incorporates all other observations that exist.

# Smaller increments ("by") and more simulations ("nsim") take more processing power but would yielf a smoother curve on the graph below.

ggplot(oil_lfun, aes(x=r2, y=obs)) +
  geom_line(color = "black") +
  geom_line(aes(x = r2, y = theo), color = "blue") +
  ggtitle("L Function Graph")

```




